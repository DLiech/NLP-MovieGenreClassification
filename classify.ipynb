{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "punctuation_list = list(string.punctuation)\n",
    "numeric_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "removed = stopwords_list + punctuation_list + numeric_list\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub('\\d+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    text = word_tokenize(text)\n",
    "    text = ' '.join(text)\n",
    "    text = nlp(text)\n",
    "    text = [w.lemma_ for w in text]\n",
    "    text = [w for w in text if w not in stopwords_list]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary mapping numerical labels to categories\n",
    "label_mapping_logreg = {1: 'Romance', 2: 'Horror', 3: 'Comedy', 4: 'Action'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category: Action\n"
     ]
    }
   ],
   "source": [
    "import pickle, os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the model from the pickle file\n",
    "model_file_path = os.path.join('Model', 'LogisticRegression.pickle')\n",
    "\n",
    "with open(model_file_path, 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "# Load the vectorizer used during training\n",
    "vectorizer_file_path = os.path.join('Model', 'vectorizer.pickle')\n",
    "with open(vectorizer_file_path, 'rb') as file:\n",
    "    vectorizer = pickle.load(file)\n",
    "\n",
    "text = input(\"Type your movie description here (Logistic Regression model).\")\n",
    "\n",
    "X_new = clean_text(text)\n",
    "X_new_transformed = vectorizer.transform([X_new])\n",
    "\n",
    "# Make predictions\n",
    "predicted_label = model.predict(X_new_transformed)[0]  # Assuming X_new_transformed is a single sample\n",
    "predicted_category = label_mapping_logreg.get(predicted_label)\n",
    "\n",
    "# Print the predicted category\n",
    "print(\"Predicted category:\", predicted_category)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary mapping numerical labels to categories\n",
    "label_mapping_bert = {0: 'Romance', 1: 'Horror', 2: 'Comedy', 3: 'Action'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at Model/BERTmodel were not used when initializing TFBertForSequenceClassification: ['dropout_39']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at Model/BERTmodel.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "modelBERT = TFBertForSequenceClassification.from_pretrained('Model/BERTmodel')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Romance\n"
     ]
    }
   ],
   "source": [
    "text = input(\"Type your movie description here (BERT model).\")\n",
    "\n",
    "textBERT = clean_text(text)\n",
    "\n",
    "# Tokenize input text\n",
    "tokens = tokenizer.encode_plus(textBERT, add_special_tokens=True, max_length=128, padding='max_length', truncation=True, return_tensors='tf')\n",
    "\n",
    "# Make predictions\n",
    "logits = modelBERT(tokens.input_ids, attention_mask=tokens.attention_mask)[0]\n",
    "\n",
    "# Convert logits to probabilities\n",
    "probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "# Get predicted class\n",
    "predicted_class = tf.argmax(probabilities, axis=1).numpy()[0]\n",
    "predicted_category = label_mapping_bert.get(predicted_class)\n",
    "\n",
    "print(\"Predicted class:\", predicted_category)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
